{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agentic AI Retrieval-Augmented Generation (RAG) on **Windows 11**\n",
        "### 2 Agents (Researcher + Answerer) • Vector DB (**FAISS**) • Local LLM via **Ollama**\n",
        "\n",
        "- Run an open model locally with **Ollama** (supports Windows, NVIDIA/AMD GPUs).\n",
        "- Use **LangChain** integrations for Ollama chat and embeddings.\n",
        "- Store and search document embeddings with **FAISS** (free, open-source).\n",
        "> **Model tip (8GB VRAM)**: Use **Llama 3.1 8B** quantized (~4.9GB Q4_K_M via Ollama).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Requirements\n",
        "- **Windows 11**; Ollama runs natively and serves API on `http://localhost:11434`.\n",
        "- **GPU optional** (NVIDIA/AMD). CPU-only works too.\n",
        "- **Memory**: target **8GB VRAM** + **64GB RAM**; Llama 3.1 8B Q4_K_M ~4.9GB.\n",
        "- **Vector DB**: **FAISS** via LangChain integration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 0 — Install Ollama (Windows) & pull models\n",
        "1. Install Ollama for Windows and environment variables for quantization\n",
        "\n",
        "    ```powershell\n",
        "    winget install --id Ollama.Ollama\n",
        "    ```\n",
        "\n",
        "    Set environment variables in Windows to enable quantization\n",
        "\n",
        "    ```powershell\n",
        "    setx OLLAMA_FLASH_ATTENTION 1\n",
        "    setx OLLAMA_KV_CACHE_TYPE \"q8_0\"\n",
        "    ```\n",
        "\n",
        "    Or, type env in the windows search field and select \"Edit the system environment variables\"\n",
        "\n",
        "2. Start Ollama, API runs at `http://localhost:11434`.\n",
        "\n",
        "    ```powershell\n",
        "    ollama serve\n",
        "    ```\n",
        "\n",
        "    Or, use the GUI. :)\n",
        "\n",
        "3. Pull quantized LLM suitable for 6GB to 8GB of VRAM:\n",
        "\n",
        "    ```powershell\n",
        "    ollama pull llama3.1:8b-instruct-q4_0\n",
        "    ```\n",
        "\n",
        "4. Pull embedding model (`nomic-embed-text` or `embeddinggemma`):\n",
        "\n",
        "    ```powershell\n",
        "    ollama pull nomic-embed-text\n",
        "    ```\n",
        "\n",
        "    or\n",
        "\n",
        "    ```powershell\n",
        "    ollama pull embeddinggemma\n",
        "    ```\n",
        "\n",
        "5. Optional: Relocate model storage via **`OLLAMA_MODELS`** env var in Windows.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 — Python environment & dependencies\n",
        "We use LangChain for ChatOllama + OllamaEmbeddings, and FAISS for vector search.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu128\n",
            "Requirement already satisfied: torch in c:\\users\\robgibson\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (2.9.1+cu128)\n",
            "Requirement already satisfied: filelock in c:\\users\\robgibson\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\robgibson\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\robgibson\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\robgibson\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\robgibson\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\robgibson\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (2025.12.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\robgibson\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (80.9.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\robgibson\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\robgibson\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "# Ensure your virtual environment is active then install dependencies\n",
        "!python -m pip install --quiet --upgrade pip\n",
        "!pip install --quiet jupyter langchain langchain-community langchain-ollama faiss-cpu langgraph python-dotenv ollama\n",
        "!pip install torch --index-url https://download.pytorch.org/whl/cu128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5812b0bb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Ensure you are running \"cuda\", not \"cpu\"\n",
        "\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 — Verify Ollama\n",
        "Ollama exposes a local API; clients connect to `http://localhost:11434`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"models\": [\n",
            "    {\n",
            "      \"model\": \"nomic-embed-text:latest\",\n",
            "      \"modified_at\": \"2025-12-14 10:13:56.860727-07:00\",\n",
            "      \"digest\": \"0a109f422b47e3a30ba2b10eca18548e944e8a23073ee3f3e947efcf3c45e59f\",\n",
            "      \"size\": 274302450,\n",
            "      \"details\": {\n",
            "        \"parent_model\": \"\",\n",
            "        \"format\": \"gguf\",\n",
            "        \"family\": \"nomic-bert\",\n",
            "        \"families\": [\n",
            "          \"nomic-bert\"\n",
            "        ],\n",
            "        \"parameter_size\": \"137M\",\n",
            "        \"quantization_level\": \"F16\"\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"model\": \"llama3.1:8b-instruct-q4_0\",\n",
            "      \"modified_at\": \"2025-12-14 08:03:50.144636-07:00\",\n",
            "      \"digest\": \"42182419e9508c30c4b1fe55015f06b65f4ca4b9e28a744be55008d21998a093\",\n",
            "      \"size\": 4661230766,\n",
            "      \"details\": {\n",
            "        \"parent_model\": \"\",\n",
            "        \"format\": \"gguf\",\n",
            "        \"family\": \"llama\",\n",
            "        \"families\": [\n",
            "          \"llama\"\n",
            "        ],\n",
            "        \"parameter_size\": \"8.0B\",\n",
            "        \"quantization_level\": \"Q4_0\"\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import json\n",
        "from ollama import Client\n",
        "\n",
        "client = Client(host='http://localhost:11434')\n",
        "tags = client.list()\n",
        "\n",
        "print(json.dumps(tags.model_dump(), indent=2, default=str)[:1000])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3 — Create a small corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f3ebd4f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Set DATA_PATH variable. Change it to match your filesystem.\n",
        "# You may get permissions issues depending on where you save it to so the Downloads\n",
        "# directory is usually a safe choice.\n",
        "\n",
        "DATA_PATH = Path(r\"C:\\Users\\RobGibson\\Downloads\\data\").resolve()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4 ['agents.txt', 'faiss_intro.txt', 'rag_overview.txt', 'windows_gpu.txt']\n"
          ]
        }
      ],
      "source": [
        "import textwrap\n",
        "\n",
        "# Ensure the folder exists\n",
        "DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Define documents using *file names only*\n",
        "docs = {\n",
        "    \"rag_overview.txt\": \"RAG couples an LLM with a retriever over external knowledge to ground responses.\",\n",
        "    \"windows_gpu.txt\": \"On Windows 11, Ollama runs natively and can accelerate models on NVIDIA/AMD GPUs when available.\",\n",
        "    \"faiss_intro.txt\": \"FAISS is a free/open-source library for efficient similarity search over dense embeddings.\",\n",
        "    \"agents.txt\": \"Two agents: Researcher retrieves context; Answerer synthesizes final response.\",\n",
        "}\n",
        "\n",
        "# Write each file into the specified path\n",
        "for filename, content in docs.items():\n",
        "    file_path = DATA_PATH / filename\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(textwrap.fill(content, width=100))\n",
        "\n",
        "# Inspect what was written\n",
        "files = list(DATA_PATH.iterdir())\n",
        "print(len(files), [p.name for p in files])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4 — Build embeddings (Ollama) & index in FAISS\n",
        "Use **OllamaEmbeddings** and **FAISS** vector store.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\RobGibson\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
            "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Indexed 4 chunks into FAISS.\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from langchain_ollama import OllamaEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Pattern includes *.txt so we actually get files\n",
        "paths = list(DATA_PATH.glob(\"*.txt\"))\n",
        "\n",
        "# If you still keep a relative 'data' folder as fallback:\n",
        "if not paths:\n",
        "    paths = list(Path(\"data\").glob(\"*.txt\"))\n",
        "\n",
        "# Build Document list safely\n",
        "raw_docs = []\n",
        "for p in paths:\n",
        "    text = p.read_text(encoding=\"utf-8\")\n",
        "    raw_docs.append(Document(page_content=text, metadata={\"source\": str(p)}))\n",
        "\n",
        "# Split, embed, and index\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = splitter.split_documents(raw_docs)\n",
        "\n",
        "emb = OllamaEmbeddings(model=\"nomic-embed-text\", base_url=\"http://localhost:11434\")\n",
        "vectorstore = FAISS.from_documents(chunks, emb)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "print(f\"Indexed {len(chunks)} chunks into FAISS.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5 — Two-agent workflow (LangGraph)\n",
        "Researcher → retrieves chunks; Answerer → generates grounded response with ChatOllama.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Two-agent workflow compiled.\n"
          ]
        }
      ],
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "from langgraph.graph import START, StateGraph\n",
        "from typing import TypedDict, List\n",
        "\n",
        "class RAGState(TypedDict):\n",
        "    question: str\n",
        "    contexts: List[str]\n",
        "    answer: str\n",
        "\n",
        "llm = ChatOllama(model='llama3.1:8b-instruct-q4_0', base_url='http://localhost:11434', temperature=0)\n",
        "\n",
        "def researcher_node(state: RAGState) -> RAGState:\n",
        "    # Retrieve top-k contexts from FAISS\n",
        "    docs = retriever.invoke(state['question'])\n",
        "    state['contexts'] = [d.page_content for d in docs]\n",
        "    return state\n",
        "\n",
        "def answerer_node(state: RAGState) -> RAGState:\n",
        "    # Generate final grounded answer\n",
        "    system = 'Use only the provided contexts. If insufficient, say you are not sure and suggest data to add.'\n",
        "    ctx = '\\n\\n'.join(state['contexts'])\n",
        "    prompt = [('system', system), ('human', 'Question: ' + state['question'] + '\\nContexts:\\n' + ctx + '\\nAnswer:')]\n",
        "    resp = llm.invoke(prompt)\n",
        "    state['answer'] = resp.content\n",
        "    return state\n",
        "\n",
        "graph = StateGraph(RAGState)\n",
        "graph.add_node('researcher', researcher_node)\n",
        "graph.add_node('answerer', answerer_node)\n",
        "graph.add_edge(START, 'researcher')\n",
        "graph.add_edge('researcher', 'answerer')\n",
        "app = graph.compile()\n",
        "print('Two-agent workflow compiled.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6 — Test the pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Retrieved contexts ---\n",
            "[1] Two agents: Researcher retrieves context; Answerer synthesizes final response. ...\n",
            "[2] RAG couples an LLM with a retriever over external knowledge to ground responses. ...\n",
            "[3] FAISS is a free/open-source library for efficient similarity search over dense embeddings. ...\n",
            "\n",
            "--- Final answer ---\n",
            "Based on the provided contexts, I'll try to answer your question.\n",
            "\n",
            "RAG stands for Retrieval-Augmented Generator. It's a technique used in natural language processing (NLP) that couples a Large Language Model (LLM) with a retriever over external knowledge to ground responses. In other words, RAG uses an LLM to generate text and a retriever to fetch relevant information from external sources, such as databases or articles.\n",
            "\n",
            "The two agents mentioned are:\n",
            "\n",
            "1. Researcher: This agent retrieves context from external knowledge sources.\n",
            "2. Answerer: This agent synthesizes the final response using the retrieved context and the Large Language Model (LLM).\n",
            "\n",
            "In this setup, the Researcher uses a retriever to fetch relevant information from external knowledge sources, which is then used by the Answerer to generate a response with the help of an LLM.\n",
            "\n",
            "The FAISS library is mentioned as a tool for efficient similarity search over dense embeddings, but it's not directly related to the question about RAG and its usage.\n"
          ]
        }
      ],
      "source": [
        "query = 'What is RAG and how are the two agents used here?'\n",
        "final_state = app.invoke({'question': query, 'contexts': [], 'answer': ''})\n",
        "print('--- Retrieved contexts ---')\n",
        "for i, c in enumerate(final_state['contexts'], 1):\n",
        "    print(f'[{i}]', c[:300], '...')\n",
        "print('\\n--- Final answer ---')\n",
        "print(final_state['answer'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea15eb6b",
      "metadata": {},
      "source": [
        "#### Tip: run the following in powershell to get information about your running models if necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fecbeb7a",
      "metadata": {
        "vscode": {
          "languageId": "powershell"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NAME                         ID              SIZE      PROCESSOR    CONTEXT    UNTIL              \n",
            "llama3.1:8b-instruct-q4_0    42182419e950    4.9 GB    100% GPU     4096       4 minutes from now    \n",
            "nomic-embed-text:latest      0a109f422b47    595 MB    100% GPU     8192       4 minutes from now    \n"
          ]
        }
      ],
      "source": [
        "!powershell -Command \"ollama ps\"\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    },
    "title": "Windows 11 RAG Tutorial: 2 Agents + Free Vector DB (FAISS) using Ollama + LangChain"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
